# Clustering con Python
El tÃ©rmino clustering hace referencia a un amplio abanico de tÃ©cnicas cuya finalidad es encontrar patrones o grupos (clusters) dentro de un conjunto de observaciones. Las particiones se establecen de forma que, las observaciones que estÃ¡n dentro de un mismo grupo, son similares entre ellas y distintas a las observaciones de otros grupos. Se trata de un mÃ©todo de aprendizaje no supervisado (unsupervised), ya que el proceso no tiene en cuenta a quÃ© grupo pertenece realmente cada observaciÃ³n (si es que existe tal informaciÃ³n). Esta caracterÃ­stica es la que diferencia al clustering de las mÃ©todos de clasificaciÃ³n en el que sÃ­ emplea la verdadera clasificaciÃ³n durante su entrenamiento.

## Medidas de distancia
Todos los mÃ©todos de clustering tienen una cosa en comÃºn, para llevar a cabo las agrupaciones necesitan definir y cuantificar la similitud entre las observaciones. El tÃ©rmino distancia se emplea dentro del contexto del clustering como cuantificaciÃ³n de la similitud o diferencia entre observaciones. Si se representan las observaciones en un espacio ğ‘ dimensional, siendo ğ‘ el nÃºmero de variables asociadas a cada observaciÃ³n, cuando mÃ¡s se asemejen dos observaciones mÃ¡s prÃ³ximas estarÃ¡n, de ahÃ­ que se emplee el tÃ©rmino distancia. La caracterÃ­stica que hace del clustering un mÃ©todo adaptable a escenarios muy diversos es que puede emplear casi cualquier tipo de distancia, lo que permite al investigador escoger la mÃ¡s adecuada para el estudio en cuestiÃ³n. A continuaciÃ³n, se describen algunas de las mÃ¡s utilizadas.

### 1. Distancia euclÃ­dea
La distancia euclÃ­dea entre dos puntos ğ‘ y ğ‘ se define como la longitud del segmento que une ambos puntos. En coordenadas cartesianas, la distancia euclÃ­dea se calcula empleando el teorema de PitÃ¡goras. Por ejemplo, en un espacio de dos dimensiones en el que cada punto estÃ¡ definido por las coordenadas (ğ‘¥,ğ‘¦), la distancia euclÃ­dea entre ğ‘ y ğ‘ es:
<img src="https://e7.pngegg.com/pngimages/801/436/png-clipart-euclidean-distance-k-nearest-neighbors-algorithm-euclidean-space-line-blue-angle.png" alt="Euclidean distance image" height="100px">

Esta ecuaciÃ³n puede generalizarse para un espacio euclÃ­deo n-dimensional donde cada punto estÃ¡ definido por un vector de n coordenadas:  ğ‘=(ğ‘1,ğ‘2,ğ‘3,...,ğ‘ğ‘›) y  ğ‘=(ğ‘1,ğ‘2,ğ‘3,...,ğ‘ğ‘›).

Una forma de dar mayor peso a aquellas observaciones que estÃ¡n mÃ¡s alejadas es emplear la distancia euclÃ­dea al cuadrado. En el caso del clustering, donde se busca agrupar observaciones que minimicen la distancia, esto se traduce en una mayor influencia de aquellas observaciones que estÃ¡n mÃ¡s distantes.

### 2. Distancia de Manhattan
La distancia de Manhattan, tambiÃ©n conocida como taxicab metric, rectilinear distance o L1 distance, define la distancia entre dos puntos ğ‘ y ğ‘ como el sumatorio de las diferencias absolutas entre cada dimensiÃ³n. Esta medida se ve menos afectada por outliers (es mÃ¡s robusta) que la distancia euclÃ­dea debido a que no eleva al cuadrado las diferencias.

<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhpnVELOqVWvtnprL4GqjGe7th-ToBmKfxZ49layLePHhQYYk-a_nRN4jKC0QSN1ieDTyYJG_DiSWisJ1BD6_2bFY4sOlVWQK-yUb9L8ZHnt1FqcKmsGYkW7gHo5DXNcvJxgHtT6DE7Id5/s1600/f%C3%B3rmula2.png" alt="Manhattan distance" height="100px">

La siguiente imagen muestra una comparaciÃ³n entre la distancia euclÃ­dea (segmento verde) y la distancia de manhattan (segmento rojo, amarillo y azul) en un espacio bidimensional. Existen mÃºltiples caminos para unir dos puntos con una misma distancia de manhattan, pero solo uno con la distancia euclÃ­dea.

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Manhattan_distance.svg/1200px-Manhattan_distance.svg.png" alt="Distancia Manhattan" height="150px">

### 3. CorrelaciÃ³n
La correlaciÃ³n es una medida de distancia muy Ãºtil cuando la definiciÃ³n de similitud se hace en tÃ©rminos de patrÃ³n o forma y no de desplazamiento o magnitud.
ğ‘‘ğ‘ğ‘œğ‘Ÿ(ğ‘,ğ‘)=1âˆ’correlacion(ğ‘,ğ‘) donde la correlaciÃ³n puede ser de distintos tipos (Pearson, Spearman, Kendall...)

En la siguiente imagen se muestra el perfil de 3 observaciones. a y b tienen exactamente el mismo patrÃ³n, pero b estÃ¡ desplazada 4 unidades por debajo de a. La observaciÃ³n c tiene un patrÃ³n distinto a las demÃ¡s, pero su rango de valores es similar al de b. Acorde a la distancia euclÃ­dea, las observaciones b y c son las mÃ¡s similares ya que la suma de la distancia al cuadrado entre sus valores es menor. Sin embargo, acorde a la correlaciÃ³n de Pearson, las observaciones mÃ¡s similares son a y b ya que siguen el mismo patrÃ³n.

![CorrelaciÃ³n](https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcRWz4mUNDkrJWfRuEDmsUJQuvzGA8BbYhQ20o6nmOLYua4yyvn9)

Este ejemplo pone de manifiesto que no existe una Ãºnica medida de distancia que sea mejor que las demÃ¡s, sino que depende del contexto en el que se utilice.

### 4. CorrelaciÃ³n Jackknife
El coeficiente de correlaciÃ³n de Pearson resulta efectivo como medida de similitud en Ã¡mbitos muy diversos. Sin embargo, tiene la desventaja de no ser robusto frente a outliers, a pesar de que se cumpla la condiciÃ³n de normalidad. Si dos variables tienen un pico o un valle comÃºn en una Ãºnica observaciÃ³n, por ejemplo, por un error de lectura, la correlaciÃ³n va a estar dominada por este registro a pesar de que entre las dos variables no exista correlaciÃ³n real. Lo mismo puede ocurrir en la direcciÃ³n opuesta. Si dos variables estÃ¡n altamente correlacionadas excepto para una observaciÃ³n, en la que los valores son muy dispares, entonces la correlaciÃ³n existente quedarÃ¡ enmascarada. Una forma de evitarlo es recurrir a la Jackknife correlation, que consiste en calcular todos los posibles coeficientes de correlaciÃ³n entre dos variables si se excluye cada vez una de las observaciones. El promedio de todas las Jackknife correlations atenÃºa en cierta medida el efecto del outlier.

### 5. Simple matching coefficient
Cuando las variables con las que se pretende determinar la similitud entre observaciones son de tipo binario, a pesar de que es posible codificarlas de forma numÃ©rica como 1 o 0, no tiene sentido aplicar operaciones aritmÃ©ticas sobre ellas (media, suma...). Por ejemplo, si la variable sexo se codifica como 1 para mujer y 0 para hombre, carece de significado decir que la media de la variable sexo en un determinado set de datos es 0.5. En situaciones como esta, no se pueden emplear medidas de similitud basadas en distancia euclÃ­dea, manhattan, correlaciÃ³n...

Dadas dos observaciones A y B, cada uno con ğ‘› atributos binarios, el simple matching coefficient (SMC) define la similitud entre ellos como:

<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSsR4xCny5EqrFV7Puy9_GqFMlgkJ0a6rKPsw&s" alt="DescripciÃ³n de la imagen" height="100px">

donde ğ‘€00 y ğ‘€11 son el nÃºmero de variables para las que ambas observaciones tienen el mismo valor (ambas 0 o ambas 1), y ğ‘€01 y ğ‘€10 el nÃºmero de variables que no coinciden. El valor de distancia simple matching distance (SMD) se corresponde con 1âˆ’ğ‘†ğ‘€ğ¶.

### 6. Ãndice Jaccard
El Ã­ndice Jaccard o coeficiente de correlaciÃ³n Jaccard es similar al simple matching coefficient (SMC). La diferencia radica en que, el SMC, tiene el tÃ©rmino ğ‘€00 en el numerador y denominador, mientras que el Ã­ndice de Jaccard no. Esto significa que SMC considera como coincidencias tanto si el atributo estÃ¡ presente en ambos sets como si el atributo no estÃ¡ en ninguno de los sets, mientras que Jaccard solo cuenta como coincidencias cuando el atributo estÃ¡ presente en ambos sets.
ğ½= (ğ‘€11) / (ğ‘€01+ğ‘€10+ğ‘€11)

o en tÃ©rminos matemÃ¡ticos de sets:

<img src="https://miro.medium.com/v2/resize:fit:648/0*ZjJjzvmmdD9QhXG2" alt="DescripciÃ³n de la imagen" height="100px">

La distancia de Jaccard (1âˆ’ğ½) supera a la simple matching distance en aquellas situaciones en las que la coincidencia de ausencia no aporta informaciÃ³n. Para ilustrar este hecho, supÃ³ngase que se quiere cuantificar la similitud entre dos clientes de un supermercado en base a los artÃ­culos comprados. Es de esperar que cada cliente solo adquiera unos pocos artÃ­culos de los muchos disponibles, por lo que el nÃºmero de artÃ­culos no comprados por ninguno (ğ‘€00) serÃ¡ muy alto. Como la distancia de Jaccard ignora las coincidencias de tipo ğ‘€00, el grado de similitud dependerÃ¡ Ãºnicamente de las coincidencias entre los artÃ­culos comprados.

### 7. Distancia Coseno
El coseno del Ã¡ngulo que forman dos vectores puede interpretarse como una medida de similitud de sus orientaciones, independientemente de sus magnitudes. Si dos vectores tienen exactamente la misma orientaciÃ³n (el Ã¡ngulo que forman es 0Âº) su coseno toma el valor de 1, si son perpendiculares (forman un Ã¡ngulo de 90Âº) su coseno es 0 y si tienen orientaciones opuestas (Ã¡ngulo de 180Âº) su coseno es de -1.

<img src="https://www.formulasexplicadas.com/wp-content/uploads/2025/01/angulo-entre-dos-vectores-formula.png" alt="FÃ³rmula Ã¡ngulo entre dos vectores" height="100px">

Si los vectores se normalizan restandoles la media (ğ‘‹âˆ’ğ‘‹Â¯), la medida recibe el nombre de coseno centrado y es equivalente a la correlaciÃ³n de Pearson.

## Escalado de las variables

La escala en la que se miden las variables y la magnitud de su varianza pueden afectar a los resultados obtenidos por clustering. Si una variable tiene una escala mucho mayor que el resto, determinarÃ¡ en gran medida el valor de distancia/similitud obtenido al comparar las observaciones, dirigiendo asÃ­ la agrupaciÃ³n final. Escalar y centrar las variables antes de calcular la matriz de distancias para que tengan media 0 y desviaciÃ³n estÃ¡ndar 1, asegura que todas las variables tengan el mismo peso cuando se realice el clustering. Sebastian Raschka hace un anÃ¡lisis muy explicativo de los distintos tipos de escalado y normalizaciÃ³n.

La forma mÃ¡s utilizada para escalar y centrar las variables es:

<img src="https://proyectodescartes.org/iCartesiLibri/materiales_didacticos/IntroduccionEstadisticaProbabilidad/4ESO/Estadistica/6_5PuntuacionesNormalizadas/img/FormulaTipificacion.png" alt="FÃ³rmula tipificaciÃ³n" height="100px">

Aplicando esta transformaciÃ³n, existe una relaciÃ³n entre la distancia euclÃ­dea y la correlaciÃ³n de Pearson que hace que los resultados obtenidos por clustering sean equivalentes.

